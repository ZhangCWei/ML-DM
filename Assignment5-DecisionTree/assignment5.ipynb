{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc9ef1a",
   "metadata": {},
   "source": [
    "**<font color = black size=6>实验五:决策树</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67a4f6",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第一部分:函数介绍</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac93628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfffb4e",
   "metadata": {},
   "source": [
    "1)Counter类的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c3787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({133: 2, 132: 1})\n",
      "0\n",
      "2\n",
      "dict_values([2, 1])\n",
      "[133, 132]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([[0,133,1],[0,132,0],[0,133,0]])\n",
    "#使用Counter类对数组第2列进行遍历\n",
    "counter = Counter(x[:,1])\n",
    "#第2列中有1个132和2个133，输出该counter对象可以统计这列的数值情况，便于之后的统计\n",
    "print(counter)\n",
    "#因为第2列中没有为0的值，所以返回0\n",
    "print(counter[0])\n",
    "#因为第2列中有2个133，所以返回2\n",
    "print(counter[133])\n",
    "#一般的字典操作方法都能在该类中使用，例如可以通过values函数返回该列的非重复值的个数，方便对某列的非重复值的个数进行查看\n",
    "print(counter.values())\n",
    "#可以输出所有非重复值\n",
    "print(list(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc27d84",
   "metadata": {},
   "source": [
    "2)使用numpy中的unique实现非重复值的提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a954859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1 132 133] [132 133]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([[0,133,1],[0,132,0],[0,133,0]])\n",
    "a=np.unique(x[:])\n",
    "a1=np.unique(x[:,1])\n",
    "print(a,a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b28b2f",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第二部分:实验任务</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91631e59",
   "metadata": {},
   "source": [
    "**<font color = blue size=3>1) 任务一</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8df3a6",
   "metadata": {},
   "source": [
    "任务一要求完成不同量化标准(信息增益, 信息增益率, 基尼系数)下的结点划分函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe46fb",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">该数据集(train_titanic.csv)为分类数据集，为泰坦尼克号的部分乘客信息以及最后是否生还。包括了四个属性特征以及一个标签(即为Survived,代表是否生还),属性特征分别为Sex(性别)，sibsp(堂兄弟妹个数)，Parch(父母与小孩的个数)，Pclass(乘客等级)  \n",
    "其中该数据集无缺失值和异常值，且所有连续变量已自动转换为离散变量,标签(Survived)也自动转变为离散变量0和1，所以你无需进行数据预处理，可以直接使用该数据集。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc6ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570e618",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">1) 使用pandas库将训练数据集'train_titanic.csv'载入到Dataframe对象中</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b1263e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sex  sibsp  Parch  Pclass  Survived\n",
      "0       0      1      0       3         0\n",
      "1       1      1      0       1         1\n",
      "2       1      0      0       3         1\n",
      "3       1      1      0       1         1\n",
      "4       0      0      0       3         0\n",
      "...   ...    ...    ...     ...       ...\n",
      "1004    0      0      0       3         0\n",
      "1005    1      0      0       1         0\n",
      "1006    0      0      0       3         0\n",
      "1007    0      0      0       3         0\n",
      "1008    0      1      1       3         0\n",
      "\n",
      "[1009 rows x 5 columns]\n",
      "     Sex  sibsp  Parch  Pclass  Survived\n",
      "0      0      0      0       1         0\n",
      "1      1      1      0       1         1\n",
      "2      1      0      0       2         0\n",
      "3      1      1      0       1         1\n",
      "4      1      1      1       2         0\n",
      "..   ...    ...    ...     ...       ...\n",
      "197    1      0      0       3         1\n",
      "198    0      1      0       3         0\n",
      "199    0      0      0       1         0\n",
      "200    1      0      2       2         1\n",
      "201    0      0      0       3         0\n",
      "\n",
      "[202 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#----your code here\n",
    "\n",
    "train_titanic = pd.read_csv(\"train_titanic.csv\")\n",
    "print(train_titanic)\n",
    "\n",
    "test_titanic = pd.read_csv(\"test_titanic.csv\")\n",
    "print(test_titanic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3bc41",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">2) 编写函数，给定任何标签数组计算其信息熵  \n",
    "    输入：标签数组  \n",
    "    输出：该数组对应的信息熵  \n",
    "    计算信息熵公式:\n",
    "某数组包含K个不同的取值，样本为第k(k=1,2,...,K)个值的数量所占比例为p_k,则其信息熵为$$Ent=-\\sum_{k=1}^K p_k log_2 p_k$$</span>\n",
    "    \n",
    "    \n",
    "<span style=\"color:purple\">例:  \n",
    "    输入:[[0],[1]]   \n",
    "    输出:(-$\\frac{1}{2}$ $log_2$ $\\frac{1}{2}$)+(-$\\frac{1}{2}$ $log_2$ $\\frac{1}{2}$)=1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa569aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算标签数组的信息熵\n",
    "def entropy(label):\n",
    "    unique_classes, counts = np.unique(label, return_counts=True)\n",
    "    total_samples = len(label)\n",
    "    entropy_value = 0\n",
    "    \n",
    "    for count in counts:\n",
    "        p_k = count / total_samples\n",
    "        entropy_value -= p_k * np.log2(p_k)\n",
    "\n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77ab84",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">3) 编写函数，函数功能为将所给的数据集按照指定维度的特征进行划分为若干个不同的数据集  \n",
    "    【输入】：特征集合，标签集合，指定维度  \n",
    "    【输出】：划分后所得到的子树属性集合，子树标记集合</span>\n",
    "\n",
    "<span style=\"color:purple\">例子:  \n",
    "    【输入】:特征集合:[[0,0,0],[0,0,1],[1,0,2]]  \n",
    "    标签集合:[[0],[1],[2]]  \n",
    "    指定维度:0</span>  \n",
    "    \n",
    "<span style=\"color:purple\">【输出】:[[0,0,0],[0,0,1]]和[[1,0,2]]  \n",
    "    [[0],[1]]和[[2]]  \n",
    "    tips:即将特征按其第0维度进行划分，由于第0维特征有0和1两个不同的数值，所以特征集合划分为[[0,0,0],[0,0,1]]和[[1,0,2]]，标签集合划分为[[0],[1]]和[[2]]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f1460ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split(features, labels, d):\n",
    "    split_feature = {}\n",
    "    split_label = {}\n",
    "\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        # 获取指定维度上的特征值\n",
    "        feature = features[i, d]\n",
    "        label = labels[i, 0]\n",
    "\n",
    "        # 如果特征值已在split_feature中,则添加到对应子集中,否则创建一个新子集\n",
    "        if feature in split_feature:\n",
    "            split_feature[feature].append(features[i])\n",
    "            split_label[feature].append(label)\n",
    "        else:\n",
    "            split_feature[feature] = [features[i]]\n",
    "            split_label[feature] = [label]\n",
    "\n",
    "    # 结果转换为numpy\n",
    "    split_feature = list(split_feature.values())\n",
    "    split_label = list(split_label.values())\n",
    "\n",
    "    return split_feature, split_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b0230f",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">4) 编写函数，函数功能为进行【一次】决策树的结点划分，遍历找出该特征集合中信息增益(使用ID3算法中的公式计算)【最大】的特征  \n",
    "    输入：特征集合，标签集合  \n",
    "    输出：该次划分的最佳信息增益值，最佳划分维度  \n",
    "    计算信息增益公式:  \n",
    "    某数据集D有若干特征值以及对应的标签值，其总样本大小为|D|,这里取其中一个特征feature,该特征包含V个不同的取值，其中值为第v(v=1,2,...,V)个值的样本数量为|$D^v$|$(\\sum_{v=1}^VD^v=|D|)$,则该特征值对应的信息增益为$$Gain(D,feature)=Ent(D)-\\sum_{v=1}^K \\frac{|D^v|}{D} Ent(D^v)$$\n",
    "该函数中你需要计算出每个特征的信息增益并输出，然后返回最佳的信息增益值和对应的特征的维数</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "242439cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算考虑属性信息后的条件熵、给定维度的条件下的信息熵\n",
    "def conditional_entropy(x_data, y_label, dimension):\n",
    "    \n",
    "    # 获取某个特征的所有可能取值\n",
    "    unique_values = np.unique(x_data[:, dimension])\n",
    "    total_samples = len(y_label)\n",
    "    cond_entropy_value = 0\n",
    "\n",
    "    for value in unique_values:\n",
    "        # 获取相应特征的标签值\n",
    "        sub_indices = np.where(x_data[:, dimension] == value)[0]\n",
    "        sub_labels = y_label[sub_indices]\n",
    "        \n",
    "        # 计算占比\n",
    "        sub_samples = len(sub_labels)\n",
    "        p = sub_samples / total_samples\n",
    "        \n",
    "        # 计算条件熵\n",
    "        sub_entropy = entropy(sub_labels)\n",
    "        cond_entropy_value += p * sub_entropy\n",
    "\n",
    "    return cond_entropy_value\n",
    "\n",
    "# 选择信息增益最大的特征输出\n",
    "def one_split_ID3(x_data, y_label):\n",
    "    num_features = x_data.shape[1]\n",
    "    total_entropy = entropy(y_label)\n",
    "    best_info_gain = 0\n",
    "    best_dimension = None\n",
    "\n",
    "    for dimension in range(num_features):\n",
    "        cond_entropy = conditional_entropy(x_data, y_label, dimension)\n",
    "        info_gain = total_entropy - cond_entropy\n",
    "\n",
    "        if info_gain > best_info_gain:\n",
    "            best_info_gain = info_gain\n",
    "            best_dimension = dimension\n",
    "        \n",
    "    return best_info_gain, best_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8593e",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">5) 编写函数，函数功能为进行【一次】决策树的结点划分，遍历找出该特征集合中信息增益率(使用C4.5算法中的公式计算)【最大】的特征  \n",
    "    输入：特征集合，标签集合  \n",
    "    输出：最佳划分的信息增益率值，对应的划分维度  \n",
    "    计算信息增益率公式:  \n",
    "    某数据集D有若干特征值以及对应的标签值，其总样本大小为|D|,这里取其中一个特征类型feature,该特征包含V个不同的取值，值为第v(v=1,2,...,V)个值的样本数量为|$D^v$|$(\\sum_{v=1}^V|D^v|=|D|)$,该特征本身的信息熵为Ent(feature),则该特征值对应的信息增益率为$$Gain\\_ratio(D,feature)=\\frac{Gain(D,feature)}{Ent(feature)}$$\n",
    "该函数中你需要输出每个特征的信息增益率，之后返回最佳的信息增益率和对应的特征维数</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d51379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IV(x_data, y_label, dimension):\n",
    "    # 获取某个特征的所有可能取值\n",
    "    unique_values = np.unique(x_data[:, dimension])\n",
    "    total_samples = len(y_label)\n",
    "    iv = 0.0\n",
    "\n",
    "    for value in unique_values:\n",
    "        # 获取相应特征的标签值\n",
    "        sub_indices = np.where(x_data[:, dimension] == value)[0]\n",
    "        sub_labels = y_label[sub_indices]\n",
    "        \n",
    "        # 计算占比\n",
    "        sub_samples = len(sub_labels)\n",
    "        p = sub_samples / total_samples\n",
    "        \n",
    "        # 计算条件熵\n",
    "        iv -= p * math.log2(p)\n",
    "        \n",
    "    return iv\n",
    "\n",
    "def one_split_C4_5(x_data, y_label):\n",
    "    num_features = x_data.shape[1]\n",
    "    total_entropy = entropy(y_label)\n",
    "    best_gain_rate = 0.0\n",
    "    best_dimension = None\n",
    "    \n",
    "    for dimension in range(num_features):\n",
    "        cond_entropy = conditional_entropy(x_data, y_label, dimension)\n",
    "        info_gain = total_entropy - cond_entropy\n",
    "        iv = IV(x_data, y_label,dimension)\n",
    "        gain_rate = info_gain / iv\n",
    "\n",
    "        if gain_rate > best_gain_rate:\n",
    "            best_gain_rate = gain_rate\n",
    "            best_dimension = dimension\n",
    "\n",
    "    return best_gain_rate, best_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40f45a",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">6) 编写函数，进行【一次】决策树的结点划分，遍历找出该特征集合中基尼系数(使用CART算法中的公式计算)最小的特征以及最佳的划分值  \n",
    "    输入：特征集合，标签集合  \n",
    "    输出：最佳的基尼系数，对应的划分维度，最佳划分值  \n",
    "    计算基尼系数公式:  \n",
    "    某数据集D有若干特征值以及对应的标签值，其总样本大小为|D|,该集合中标签类别总共有K类，第k类样本所占比例为$p_k$(k=1,2,..,K),则该数据集对应的基尼系数为\n",
    "    $$Gini(D)=1-\\sum_{k=1}^K{p_k}^2$$  \n",
    "    而取其中一个特征feature，选定该特征的一个值value，根据该特征的值是否为value将数据集分为两个子集$D_1$和$D_2$,则该特征对应的基尼系数为$$Gini\\_index(D,feature)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$$\n",
    "该函数中你需要遍历每一列特征，找出每列中的非重复值，计算出每个非重复值的基尼系数，返回最小的基尼系数、对应的特征维数和非重复值（分类值）。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d073721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(y):\n",
    "    # 计算基尼系数\n",
    "    unique_classes, counts =np.unique(y, return_counts=True)\n",
    "    total_samples = len(y)\n",
    "    gini = 1.0\n",
    "    for count in counts:\n",
    "        p = count / total_samples\n",
    "        gini -= p**2\n",
    "    return gini\n",
    "\n",
    "def one_split_CART(x_data, y_label):\n",
    "    best_gini = 1.0\n",
    "    best_dimension = None\n",
    "    best_value = None\n",
    "\n",
    "    for dimension in range(x_data.shape[1]):\n",
    "        unique_values = np.unique(x_data[:, dimension])\n",
    "        for value in unique_values:\n",
    "            # 根据特征值划分数据集\n",
    "            left = x_data[:, dimension] <= value\n",
    "            right = x_data[:, dimension] > value\n",
    "\n",
    "            # 计算各部分的基尼系数\n",
    "            left_gini = Gini(y_label[left])\n",
    "            right_gini = Gini(y_label[right])\n",
    "\n",
    "            # 计算加权基尼系数\n",
    "            gini = (left_gini * np.sum(left) + right_gini * np.sum(right)) / len(y_label)\n",
    "\n",
    "            # 判断是否为最佳\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_dimension = dimension\n",
    "                best_value = value\n",
    "    \n",
    "    return best_gini, best_dimension, best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdc9f0",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">7) 应用之前你在第4、5、6个部分编写的三个函数，在训练数据集'train_titanic.csv'上依次使用这些函数进行【一次】结点划分，并输出对应的最佳特征维数以及相应的信息增益值/信息增益率/(基尼系数和分类值)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8769cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息增益的最佳特征维度为： 0\n",
      "信息增益率的最佳特征维度为： 0\n",
      "基尼系数最佳特征维度为： 0\n",
      "信息增益值 0.10750711887455178\n",
      "信息增益率 0.11339165967945304\n",
      "基尼系数 0.2964915724641573\n",
      "分类值 0\n"
     ]
    }
   ],
   "source": [
    "#----your code here\n",
    "x_data = train_titanic.drop(columns=['Survived']).values\n",
    "y_label = train_titanic['Survived'].values\n",
    "\n",
    "best_info_gain, best_dimension_ID3 = one_split_ID3(x_data, y_label)\n",
    "best_gain_rate, best_dimension_C4_5 = one_split_C4_5(x_data, y_label)\n",
    "best_gini, best_dimension_CART, best_value = one_split_CART(x_data, y_label)\n",
    "\n",
    "print(\"信息增益的最佳特征维度为：\", best_dimension_ID3)\n",
    "print(\"信息增益率的最佳特征维度为：\", best_dimension_C4_5)\n",
    "print(\"基尼系数最佳特征维度为：\", best_dimension_CART)\n",
    "\n",
    "print(\"信息增益值\", best_info_gain)\n",
    "print(\"信息增益率\", best_gain_rate)\n",
    "print(\"基尼系数\", best_gini)\n",
    "print(\"分类值\", best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df259d58",
   "metadata": {},
   "source": [
    "**<font color = blue size=3>2) 任务二</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5fe4ac",
   "metadata": {},
   "source": [
    "任务二承接任务一，用【ID3】算法实现一棵完整的决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67d941",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">1) 整理任务一的代码，根据实验二要求做适当修改来编写函数，【从剩余的特征集A中】寻找使得信息增益最大的特征   \n",
    "    输入：数据集D、剩余的特征集A    \n",
    "    输出：最佳划分的特征维数    \n",
    "    计算信息增益公式:  \n",
    "    某数据集D有若干特征值以及对应的标签值，其总样本大小为|D|,这里取其中一个特征feature,该特征包含V个不同的取值，特征值为第v(v=1,2,...,V)个值的样本数量为|$D^v$|$(\\sum_{v=1}^VD^v=|D|)$,则该特征对应的信息增益为$$Gain(D,feature)=Ent(D)-\\sum_{v=1}^K \\frac{|D^v|}{D} Ent(D^v)$$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf436ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(D, A):\n",
    "    best_dimension = None\n",
    "    best_info_gain = 0\n",
    "    \n",
    "    for dimension in A:\n",
    "        x_data = D[:, dimension].reshape(-1, 1)\n",
    "        info_gain, _ = one_split_ID3(x_data, D[:, -1])\n",
    "        \n",
    "        if info_gain > best_info_gain:\n",
    "            best_info_gain = info_gain\n",
    "            best_dimension = dimension\n",
    "\n",
    "    return best_dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ceb0dc",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">2) 完成DTree类中的TreeGenerate、train函数以完成决策树的构建。并完成DTree类中的predict函数来用构建好的决策树来对测试数据集进行预测并输出预测准确率。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9473e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 树结点类\n",
    "class Node:\n",
    "    def __init__(self, isLeaf=True, label=-1, feature_index=-1):\n",
    "        self.isLeaf = isLeaf                # isLeaf表示该结点是否是叶结点\n",
    "        self.label = label                  # label表示该叶结点的label(当结点为叶结点时有用)\n",
    "        self.feature_index = feature_index  # feature_index表示该分支结点的划分特征的序号(当结点为分支结点时有用)\n",
    "        self.children = {}                  # children表示该结点的所有孩子结点，dict类型，方便进行决策树的搜索\n",
    "        \n",
    "    def addNode(self, val, node):\n",
    "        self.children[val] = node           #为当前结点增加一个划分特征的值为val的孩子结点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12a28664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树类\n",
    "class DTree:\n",
    "    def __init__(self):\n",
    "        self.tree_root = None       # 决策树的根结点\n",
    "        self.possible_value = {}    # 用于存储每个特征可能的取值\n",
    "    \n",
    "    '''\n",
    "    TreeGenerate函数用于递归构建决策树,伪代码参照课件中的“Algorithm 1 决策树学习基本算法”\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    def TreeGenerate(self, D, A):\n",
    "        node = Node(isLeaf=False)\n",
    "\n",
    "        # if D中样本全属于同一类别C then\n",
    "        if len(np.unique(D[:, -1])) == 1:\n",
    "            node.isLeaf = True\n",
    "            node.label = D[0, -1]  # 叶结点的类别为D中唯一的类别\n",
    "            return node\n",
    "        # end if\n",
    "\n",
    "        # if A = Ø OR D中样本在A上取值相同 then\n",
    "        if len(A) == 0 or len(np.unique(D[:, list(A)])) == 1:\n",
    "            node.isLeaf = True\n",
    "            node.label = np.argmax(np.bincount(D[:, -1]))  # 叶结点的类别为D中样本最多的类\n",
    "            return node\n",
    "        # end if\n",
    "\n",
    "        # 从A中选择最优划分特征a_star\n",
    "        a_star = best_split(D, A)\n",
    "        node.feature_index = a_star  # 设置分支节点的划分特征\n",
    "\n",
    "        # for a_star 的每一个值a_star_v do\n",
    "        for a_star_v in self.possible_value[a_star]:\n",
    "            D_v = D[D[:, a_star] == a_star_v]\n",
    "\n",
    "            if len(D_v) == 0:\n",
    "                child_node = Node(isLeaf=True, label=np.argmax(np.bincount(D[:, -1])))\n",
    "            else:\n",
    "                A_new = A - {a_star}\n",
    "                child_node = self.TreeGenerate(D_v, A_new)\n",
    "\n",
    "            node.addNode(a_star_v, child_node)\n",
    "        # end for\n",
    "\n",
    "        return node\n",
    "      \n",
    " \n",
    "    '''\n",
    "    train函数可以做一些数据预处理(比如Dataframe到numpy矩阵的转换,提取属性集等),并调用TreeGenerate函数来递归地生成决策树\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def train(self, D):\n",
    "        # 将Dataframe对象转换为numpy矩阵（也可以不转，自行决定做法）\n",
    "        D = np.array(D) \n",
    "        \n",
    "        # 特征集A\n",
    "        A = set(range(D.shape[1] - 1))\n",
    "        \n",
    "        #记下每个特征可能的取值\n",
    "        for every in A:\n",
    "            self.possible_value[every] = np.unique(D[:, every])\n",
    "\n",
    "        # 递归地生成决策树，并将决策树的根结点赋值给self.tree_root\n",
    "        self.tree_root = self.TreeGenerate(D, A)\n",
    "        \n",
    "        return\n",
    "\n",
    "    \n",
    "    '''\n",
    "    predict函数对测试集D进行预测,并输出预测准确率(预测正确的个数 / 总数据数量)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def predict(self, D):\n",
    "        D = np.array(D)\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i in range(len(D)):\n",
    "            current_node = self.tree_root\n",
    "\n",
    "            while not current_node.isLeaf:\n",
    "                feature_index = current_node.feature_index\n",
    "                feature_value = D[i, feature_index]\n",
    "\n",
    "                if feature_value in current_node.children:\n",
    "                    current_node = current_node.children[feature_value]\n",
    "                else:\n",
    "                    # 若测试数据包含了训练集中未出现的特征值，做一种简单的处理，将其分类到出现次数最多的类别\n",
    "                    current_node = Node(isLeaf=True, label=np.argmax(np.bincount(D[:, -1])))\n",
    "\n",
    "            predicted_label = current_node.label\n",
    "            actual_label = D[i, -1]\n",
    "\n",
    "            if predicted_label == actual_label:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        accuracy = correct_predictions / len(D)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d647f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率: 0.837\n"
     ]
    }
   ],
   "source": [
    "#----your code here\n",
    "\n",
    "# 构建决策树\n",
    "decision_tree = DTree()\n",
    "\n",
    "# 训练决策树\n",
    "decision_tree.train(train_titanic)\n",
    "\n",
    "# 利用构建好的决策树对测试数据集进行预测，输出预测准确率\n",
    "accuracy = decision_tree.predict(test_titanic)\n",
    "\n",
    "print(f\"预测准确率: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e214a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch Node: feature_index = 0\n",
      "|--> Child Value: 0\n",
      "   Branch Node: feature_index = 2\n",
      "   |--> Child Value: 0\n",
      "      Branch Node: feature_index = 3\n",
      "      |--> Child Value: 1\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 2\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 3\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 0\n",
      "   |--> Child Value: 1\n",
      "      Branch Node: feature_index = 1\n",
      "      |--> Child Value: 0\n",
      "         Branch Node: feature_index = 3\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 1\n",
      "         Branch Node: feature_index = 3\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 2\n",
      "         Leaf Node: label = 1\n",
      "      |--> Child Value: 3\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 4\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 5\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 8\n",
      "         Leaf Node: label = 0\n",
      "   |--> Child Value: 2\n",
      "      Branch Node: feature_index = 1\n",
      "      |--> Child Value: 0\n",
      "         Branch Node: feature_index = 3\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 1\n",
      "         Branch Node: feature_index = 3\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 2\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 3\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 4\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 5\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 8\n",
      "         Leaf Node: label = 0\n",
      "   |--> Child Value: 3\n",
      "      Leaf Node: label = 0\n",
      "   |--> Child Value: 4\n",
      "      Leaf Node: label = 0\n",
      "   |--> Child Value: 5\n",
      "      Leaf Node: label = 0\n",
      "   |--> Child Value: 6\n",
      "      Leaf Node: label = 0\n",
      "   |--> Child Value: 9\n",
      "      Leaf Node: label = 0\n",
      "|--> Child Value: 1\n",
      "   Branch Node: feature_index = 3\n",
      "   |--> Child Value: 1\n",
      "      Branch Node: feature_index = 2\n",
      "      |--> Child Value: 0\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 1\n",
      "      |--> Child Value: 1\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 1\n",
      "      |--> Child Value: 2\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 1\n",
      "      |--> Child Value: 3\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 4\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 5\n",
      "         Leaf Node: label = 1\n",
      "      |--> Child Value: 6\n",
      "         Leaf Node: label = 1\n",
      "      |--> Child Value: 9\n",
      "         Leaf Node: label = 1\n",
      "   |--> Child Value: 2\n",
      "      Branch Node: feature_index = 1\n",
      "      |--> Child Value: 0\n",
      "         Branch Node: feature_index = 2\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 6\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 9\n",
      "            Leaf Node: label = 1\n",
      "      |--> Child Value: 1\n",
      "         Branch Node: feature_index = 2\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 6\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 9\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 2\n",
      "         Branch Node: feature_index = 2\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 6\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 9\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 3\n",
      "         Leaf Node: label = 1\n",
      "      |--> Child Value: 4\n",
      "         Leaf Node: label = 1\n",
      "      |--> Child Value: 5\n",
      "         Leaf Node: label = 1\n",
      "      |--> Child Value: 8\n",
      "         Leaf Node: label = 1\n",
      "   |--> Child Value: 3\n",
      "      Branch Node: feature_index = 2\n",
      "      |--> Child Value: 0\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 1\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 2\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 3\n",
      "         Leaf Node: label = 1\n",
      "      |--> Child Value: 4\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 5\n",
      "         Branch Node: feature_index = 1\n",
      "         |--> Child Value: 0\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 1\n",
      "            Leaf Node: label = 1\n",
      "         |--> Child Value: 2\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 3\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 4\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 5\n",
      "            Leaf Node: label = 0\n",
      "         |--> Child Value: 8\n",
      "            Leaf Node: label = 0\n",
      "      |--> Child Value: 6\n",
      "         Leaf Node: label = 0\n",
      "      |--> Child Value: 9\n",
      "         Leaf Node: label = 0\n"
     ]
    }
   ],
   "source": [
    "#展示生成的决策树结构\n",
    "def display_tree(node, indent=''):\n",
    "    if node.isLeaf:\n",
    "        print(indent + \"Leaf Node: label =\", node.label)\n",
    "    else:\n",
    "        print(indent + \"Branch Node: feature_index =\", node.feature_index)\n",
    "        for value, child in node.children.items():\n",
    "            print(indent + \"|--> Child Value:\", value)\n",
    "            display_tree(child, indent + \"   \")\n",
    "\n",
    "\n",
    "display_tree(decision_tree.tree_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7876bb",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第三部分:作业提交</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82677693",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">本次实验分两周完成,实验报告提交地址(学号+姓名+实验五):https://send2me.cn/s0Yr-w4z/QCKd4ibz34miqQ   截止日期:10.20 14:20</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95691245",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">实验课件获取地址:https://www.jianguoyun.com/p/DZRe9cwQp5WhChjz4p8FIAA</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
